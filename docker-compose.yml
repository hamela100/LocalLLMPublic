name: local_llm

services:
  # =======================
  # vLLM - Chat
  # =======================
  vllm-chat:
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TRANSFORMERS_OFFLINE=1  # <-- Add this line when going offline to stop HF calls. Turn off when needing to call a new chat model.
      #- HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    command:
      - "--model=${VLLM_CHAT_MODEL}"
      - "--dtype=${VLLM_CHAT_DTYPE}"  
      - "--gpu-memory-utilization=${VLLM_CHAT_GPU_UTIL}"
      - "--port=8000"
      #- "--max-model-len=${VLLM_CHAT_MAX_MODEL_LEN}" 
    volumes:
      - hf_cache:/root/.cache/huggingface
    networks:
      - ai-net

  # =======================
  # vLLM - Embeddings
  # =======================
  vllm-embed:
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TRANSFORMERS_OFFLINE=1  # <-- Add this line when going offline to stop HF calls. Turn off when needing to call a new embed model.
      #- HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    command:
      - "--runner=pooling"
      - "--convert=embed"
      - "--model=${VLLM_EMBED_MODEL}"
      - "--dtype=${VLLM_EMBED_DTYPE}"
      - "--gpu-memory-utilization=${VLLM_EMBED_GPU_UTIL}"
      - "--port=8001"
      - "--trust-remote-code"  # <-- Keep this even in offline mode. Transformers offline will keep it from accessing HF.

    volumes:
      - hf_cache:/root/.cache/huggingface
    networks:
      - ai-net

  # =======================
  # Docling (Document Parser, CPU version (change when onto workstation))
  # =======================
  docling:
    image: ghcr.io/docling-project/docling-serve-cpu:v1.3.0
    restart: unless-stopped
    environment:
      - DOCLING_SERVE_ENABLE_UI=1   # enables the /ui playground
    ports:
      - "127.0.0.1:5001:5001"       # host-only; not exposed to LAN
    volumes:
      - hf_cache:/root/.cache/huggingface #share huggingface cache so not downloading models twice
    networks:
      - ai-net

  # =======================
  # Qdrant (Vector DB)
  # =======================
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    volumes:
      - qdrant_storage:/qdrant/storage
    networks:
      - ai-net
      
  # =======================
  # OpenWebUI (LAN access)
  # =======================
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    depends_on:
      - qdrant
      - vllm-chat
      - vllm-embed
      - docling
    environment:
      - OPENAI_API_BASE_URL=http://vllm-chat:8000/v1
      - OPENAI_API_KEY=local-key
      - VECTOR_DB=qdrant
      - QDRANT_URI=${QDRANT_URI:-http://qdrant:6333}
      - WEBUI_NAME=${WEBUI_NAME}
    volumes:
      - openwebui_data:/app/backend/data
      - openwebui_cache:/root/.cache
    ports:
      - "0.0.0.0:3000:8080"   # reachable from host + LAN
    networks:
      - ai-net

networks:
  ai-net: {}

volumes:
  openwebui_data:
  openwebui_cache:
  qdrant_storage:
  hf_cache:
